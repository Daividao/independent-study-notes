{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMFtLGQqTWxH"
      },
      "source": [
        "## Takeaway from this homework\n",
        "* Trigram NN loss is better than Bigram NN loss.\n",
        "* The current Trigram NN solution is unable to achieve the same loss as the couting solution. It's possible just my bugs...\n",
        "* Instead of using 1-hot encoding to multiply xs with the NN weight (W), we can just index into rows of W directly\n",
        "* Use F.cross_entropy instead of manually calculate loss like from ex1 to ex4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfnrrk_pQ132"
      },
      "source": [
        "# Setup\n",
        "\n",
        "* Watch https://youtu.be/PaCmpygFfXo?si=6_QhkMrB9g09OEpw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "SZL0_EQvUM5c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7Dpstn9QrOt",
        "outputId": "488cb65f-7908-4731-d400-ba90e5cdc688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount this colab to the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8XEziCrQ6er",
        "outputId": "eb5f9ab1-8509-4013-ae7a-81161b7d79e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks'\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "oxW8wzkHQSNC"
      },
      "outputs": [],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvVDS_JcTn2G",
        "outputId": "95219f4e-83a3-482d-fd5a-89d9b05a5779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMs5XQj8Uynw",
        "outputId": "edb6d120-53dd-471c-a0b5-95c9cb6d2b70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0rBzoDS63aN"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "0D4KW4Js68cs"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "bigram_input_size = 27\n",
        "bigram_output_size = 27\n",
        "trigram_input_size = 27 * 27\n",
        "trigram_output_size = 27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3V-Ts6E5gDt"
      },
      "source": [
        "## Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "OfFX8Rks5mB9"
      },
      "outputs": [],
      "source": [
        "def stoi(c):\n",
        "  \"\"\"Maps 1 character to its index\"\"\"\n",
        "  ord1 = 0 if c == '.' else ord(c) - ord('a') + 1\n",
        "  if not (0 <= ord1 < 27 and 0 <= ord1 <= 27):\n",
        "    raise ValueError(\"Invalid characters\")\n",
        "  return ord1\n",
        "\n",
        "def itos(v):\n",
        "  \"\"\"Maps an index to 1 character\"\"\"\n",
        "  if not (0 <= v < 27):\n",
        "    raise ValueError(\"Invalid index number\")\n",
        "  return '.' if v == 0 else chr(ord('a') + v - 1)\n",
        "\n",
        "# We will need a 2-characters index as we need to encode 2 characters\n",
        "def sstoi(c1, c2):\n",
        "  \"\"\"Maps 2 characters to its index\"\"\"\n",
        "  return stoi(c1) * 27 + stoi(c2)\n",
        "\n",
        "def itoss(v):\n",
        "  \"\"\"Maps an index to 2 characters\"\"\"\n",
        "  if not (0 <= v < 27 * 27):\n",
        "    raise ValueError(\"Invalid index number\")\n",
        "  ord1, ord2 = v // 27, v % 27\n",
        "  return itos(ord1) + itos(ord2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raHf10uI5pAC"
      },
      "source": [
        "## Core ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "QrQtSR9W5Qtz"
      },
      "outputs": [],
      "source": [
        "def split_dataset(xs, ys, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1):\n",
        "  \"\"\"Splits dataset into train, dev, test\"\"\"\n",
        "\n",
        "  # Ensure the ratios sum to 1\n",
        "  assert abs(train_ratio + dev_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
        "\n",
        "  # Create a TensorDataset\n",
        "  dataset = TensorDataset(xs, ys)\n",
        "\n",
        "  # Calculate the sizes for each split\n",
        "  total_size = len(dataset)\n",
        "  train_size = int(train_ratio * total_size)\n",
        "  dev_size = int(dev_ratio * total_size)\n",
        "  test_size = total_size - train_size - dev_size  # Ensure sizes sum to total_size\n",
        "\n",
        "  # Split the dataset\n",
        "  train_dataset, dev_dataset, test_dataset = random_split(\n",
        "      dataset, [train_size, dev_size, test_size]\n",
        "  )\n",
        "\n",
        "  # Extract xs and ys from each split\n",
        "  train_xs, train_ys = train_dataset[:]\n",
        "  dev_xs, dev_ys = dev_dataset[:]\n",
        "  test_xs, test_ys = test_dataset[:]\n",
        "\n",
        "  return dict({\n",
        "    'train': {'xs': train_xs, 'ys': train_ys},\n",
        "    'dev': {'xs': dev_xs, 'ys': dev_ys},\n",
        "    'test': {'xs': test_xs, 'ys': test_ys}\n",
        "  })\n",
        "\n",
        "def _calculate_logits(xs, W):\n",
        "  xenc = F.one_hot(xs, num_classes=W.shape[0]).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  return logits\n",
        "\n",
        "def _calculate_probs(xs, W):\n",
        "  logits = _calculate_logits(xs, W)\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  return probs\n",
        "\n",
        "def _loss(ys, probs):\n",
        "  return -probs[torch.arange(ys.nelement()), ys].log().mean()\n",
        "\n",
        "def _forward(xs, ys, W, reg_matrix=None):\n",
        "  \"\"\"Neural net forward pass\"\"\"\n",
        "\n",
        "  logits = _calculate_logits(xs, W)\n",
        "  probs = _calculate_probs(logits)\n",
        "  loss = _loss(ys, probs)\n",
        "  if reg_matrix is not None:\n",
        "    loss += reg_matrix\n",
        "  return loss\n",
        "\n",
        "  return _forward(xs, ys, W).item()\n",
        "\n",
        "def loss(xs, ys, W):\n",
        "  return _forward(xs, ys, W).item()\n",
        "\n",
        "def train(xs, ys, W, passes, model_name, reg_rate=0.01, verbose = False):\n",
        "  \"\"\"Trains model\"\"\"\n",
        "\n",
        "  print(f'Training model {model_name}')\n",
        "\n",
        "  # Gradient descent\n",
        "  for k in range(passes):\n",
        "\n",
        "    # forward pass\n",
        "    loss = _forward(xs, ys, W, reg_rate*(W**2).mean())\n",
        "    if verbose:\n",
        "      print(loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50 * W.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeR9yCZLSfn3"
      },
      "source": [
        "## Bigram library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "h336pGKUZVQa"
      },
      "outputs": [],
      "source": [
        "def get_bigram_dataset(words):\n",
        "  \"\"\"Converts a list of words into a mapping from a char to the next one.\"\"\"\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "      ix1, ix2 = stoi(ch1), stoi(ch2)\n",
        "      xs.append(ix1)\n",
        "      ys.append(ix2)\n",
        "  xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
        "  return xs, ys\n",
        "\n",
        "def sample_bigram(W, sample_size=5):\n",
        "  \"\"\"Samples words from the bigram model\"\"\"\n",
        "  for i in range(sample_size):\n",
        "    ix = 0\n",
        "    word_builder = ['.']\n",
        "    while True:\n",
        "      p = _calculate_probs(torch.tensor([ix]), W)\n",
        "\n",
        "      # Get the next character\n",
        "      ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "\n",
        "      # Append the character to the word\n",
        "      word_builder.append(itos(ix))\n",
        "      if ix == 0:\n",
        "        break\n",
        "    print(''.join(word_builder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeHs_niJSozc"
      },
      "source": [
        "## Trigram library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "DjI4VDBySrFI"
      },
      "outputs": [],
      "source": [
        "def get_trigram_dataset(words):\n",
        "  \"\"\"Converts a list of words into a mapping from 2 chars to the next one.\"\"\"\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for i in range(2, len(chs)):\n",
        "      c0, c1, c2 = chs[i-2], chs[i-1], chs[i]\n",
        "      xs.append(sstoi(c0, c1))\n",
        "      ys.append(stoi(c2))\n",
        "  xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
        "  return xs, ys\n",
        "\n",
        "def sample_trigram(W, sample_size=5):\n",
        "  \"\"\"Samples words from the trigram model\"\"\"\n",
        "  for i in range(sample_size):\n",
        "    ii = torch.randint(0, 27, (1,)).item()\n",
        "    word_builder = list(itoss(ii))\n",
        "    while True:\n",
        "      p = _calculate_probs(torch.tensor([ii]), W)\n",
        "\n",
        "      # Get the next character\n",
        "      i = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "\n",
        "      # Get previous 2 characters\n",
        "      ii = sstoi(itoss(ii)[1], itos(i))\n",
        "\n",
        "      # Append the character to the word\n",
        "      word_builder.append(itos(i))\n",
        "      if i == 0:\n",
        "        break\n",
        "    print(''.join(word_builder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkM-ULbGMTww"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "* Train a trigram language model, i.e. take two characters as an input to predict the 3rd one.\n",
        "\n",
        "* Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model? **Answer**: Yes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgtmOoNIOgP"
      },
      "source": [
        "## Bigram Counting Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "c9YKOi_5ISxR"
      },
      "outputs": [],
      "source": [
        "# Compute all frequencies\n",
        "N = torch.zeros((bigram_input_size, bigram_output_size), dtype=torch.int32)\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1, ix2 = stoi(ch1), stoi(ch2)\n",
        "    N[ix1, ix2] += 1\n",
        "\n",
        "# Smoothens N so no frequencies are zeroes\n",
        "P = (N+1).float()\n",
        "\n",
        "# Calculates Probability Distribution for P\n",
        "P = P / P.sum(1, keepdim = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwB3RGoQJXrH",
        "outputId": "d07f0373-acb1-4664-f235-ea65a06ff693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counting solution loss\n",
            "log_likelihood=tensor(-559951.5625)\n",
            "nll=tensor(559951.5625)\n",
            "2.4543561935424805\n"
          ]
        }
      ],
      "source": [
        "# Evaluate loss\n",
        "\n",
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi(ch1)\n",
        "    ix2 = stoi(ch2)\n",
        "    prob = P[ix1, ix2]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "\n",
        "print(\"Counting solution loss\")\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6IqEDg_Isr9",
        "outputId": "77bf1855-cd80-437e-d775-db98b30e4704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".junide.\n",
            ".janasah.\n",
            ".p.\n",
            ".cony.\n",
            ".a.\n"
          ]
        }
      ],
      "source": [
        "# Try sampling a few words from the distribution\n",
        "for i in range(5):\n",
        "  ix = 0\n",
        "  word_builder = [itos(ix)]\n",
        "  while True:\n",
        "    p = P[ix]\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    word_builder.append(itos(ix))\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(word_builder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ1Km0rO6XtB"
      },
      "source": [
        "## Trigram Counting Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "JgSe0k5c7bq_"
      },
      "outputs": [],
      "source": [
        "# Compute all frequencies\n",
        "N = torch.zeros((trigram_input_size, trigram_output_size), dtype=torch.int32)\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for i in range(2, len(chs)):\n",
        "    c0, c1, c2 = chs[i-2], chs[i-1], chs[i]\n",
        "    N[sstoi(c0, c1), stoi(c2)] += 1\n",
        "\n",
        "# Smoothens N so no frequencies are zeroes\n",
        "P = (N+1).float()\n",
        "\n",
        "# Calculates Probability Distribution for P\n",
        "P = P / P.sum(1, keepdim = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtQNkWF77sod",
        "outputId": "0cf5d246-0a48-4631-98f1-565c69bbd319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Couting solution loss\n",
            "log_likelihood=tensor(-410414.9688)\n",
            "nll=tensor(2.0927)\n",
            "1.0671130439732224e-05\n"
          ]
        }
      ],
      "source": [
        "# Evaluate loss\n",
        "\n",
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for i in range(2, len(chs)):\n",
        "    c0, c1, c2 = chs[i-2], chs[i-1], chs[i]\n",
        "    prob = P[sstoi(c0, c1), stoi(c2)]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "\n",
        "print(\"Couting solution loss\")\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood / n\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uHFemaT7kTv",
        "outputId": "296ee490-9ef1-413b-e375-49a8933fa9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".anna.\n",
            ".adi.\n",
            ".arltoper.\n",
            ".amaree.\n",
            ".aviahnia.\n"
          ]
        }
      ],
      "source": [
        "# Try sampling a few words from the distribution\n",
        "for i in range(5):\n",
        "  word_builder = ['.', 'a']\n",
        "\n",
        "  # Starts with <start> + a\n",
        "  ii = sstoi('.', 'a')\n",
        "  while True:\n",
        "    # Gets the probability distribution for the 2 characters\n",
        "    p = P[ii]\n",
        "\n",
        "    # Get the next character\n",
        "    i = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "\n",
        "    # Append the character to the word\n",
        "    word_builder.append(itos(i))\n",
        "\n",
        "    # Get previous 2 characters\n",
        "    ii = sstoi(itoss(ii)[1], itos(i))\n",
        "\n",
        "    if i == 0:\n",
        "      break\n",
        "\n",
        "  print(''.join(word_builder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4njZjqObX6"
      },
      "source": [
        "## Bigram Neural Net Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofnb_Bj0Od5A",
        "outputId": "f7379fbe-b7f5-4d0b-86a3-1065e596a625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model bigram\n",
            "Neural network solution loss\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.461170196533203"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs2, ys2 = get_bigram_dataset(words)\n",
        "split2 = split_dataset(xs2, ys2, 1, 0, 0)\n",
        "W2 = torch.randn((bigram_input_size, bigram_output_size), generator=g, requires_grad=True)\n",
        "train(split2['train']['xs'], split2['train']['ys'], W2, 500, 'bigram')\n",
        "print(\"Neural network solution loss\")\n",
        "loss(xs2, ys2, W2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxtN81fERqT9"
      },
      "source": [
        "## Trigram Neural Net Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2hwa3-3RscI",
        "outputId": "a02d254d-85c2-45ff-ebb6-a58859b8fa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model trigram\n",
            "Neural network solution loss\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.1555898189544678"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs3, ys3 = get_trigram_dataset(words)\n",
        "split3 = split_dataset(xs3, ys3, 1, 0, 0)\n",
        "W3 = torch.randn((trigram_input_size, trigram_output_size), generator=g, requires_grad=True)\n",
        "train(split3['train']['xs'], split3['train']['ys'], W3, 500, 'trigram')\n",
        "print(\"Neural network solution loss\")\n",
        "loss(xs3, ys3, W3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgB29T9XiVjw"
      },
      "source": [
        "# Excercise 2\n",
        "\n",
        "* Split up the dataset randomly into 80% train set, 10% dev set, 10% test set.\n",
        "* Train the bigram and trigram models only on the training set.\n",
        "* Evaluate them on dev and test splits. What can you see? **Answer:** trigram is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7CiOPow4oL"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "BXnG0b46m-jd"
      },
      "outputs": [],
      "source": [
        "xs2, ys2 = get_bigram_dataset(words)\n",
        "xs3, ys3 = get_trigram_dataset(words)\n",
        "split2 = split_dataset(xs2, ys2)\n",
        "split3 = split_dataset(xs3, ys3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxS3OSzfw8CA"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9F9PW1Bkiq1",
        "outputId": "cc323e35-6cbc-4a40-bf65-ff7884a0f1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model bigram\n",
            "Training model trigram\n"
          ]
        }
      ],
      "source": [
        "passes = 500\n",
        "\n",
        "# train bigram\n",
        "W2 = torch.randn((bigram_input_size, bigram_output_size), generator=g, requires_grad=True)\n",
        "train(split2['train']['xs'], split2['train']['ys'], W2, passes, 'bigram')\n",
        "\n",
        "# train trigram\n",
        "W3 = torch.randn((trigram_input_size, trigram_output_size), generator=g, requires_grad=True)\n",
        "train(split3['train']['xs'], split3['train']['ys'], W3, passes, 'trigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSLTmryxF7a"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUEr84GUp83T",
        "outputId": "49af4247-4099-4d50-f557-fcb61ae959a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing bigram vs trigram loss on train\n",
            "bigram:  2.4602653980255127\n",
            "trigram:  2.1518821716308594\n",
            "Comparing bigram vs trigram loss on dev\n",
            "bigram:  2.467137098312378\n",
            "trigram:  2.187124252319336\n",
            "Comparing bigram vs trigram loss on test\n",
            "bigram:  2.4631457328796387\n",
            "trigram:  2.189754009246826\n"
          ]
        }
      ],
      "source": [
        "for dataset in ['train', 'dev', 'test']:\n",
        "  print(f'Comparing bigram vs trigram loss on {dataset}')\n",
        "  print(\"bigram: \", loss(split2[dataset]['xs'], split2[dataset]['ys'], W2))\n",
        "  print(\"trigram: \", loss(split3[dataset]['xs'], split3[dataset]['ys'], W3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEus0ETsaNde"
      },
      "source": [
        "## Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "228fNbLhaPFT",
        "outputId": "940b6834-4234-473b-c398-932275a04535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram samples:\n",
            ".stojafae.\n",
            ".ll.\n",
            ".nama.\n",
            ".ermryny.\n",
            ".ry.\n",
            "\n",
            "Trigram samples:\n",
            ".ka.\n",
            ".no.\n",
            ".ham.\n",
            ".sa.\n",
            ".prapfgziana.\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram samples:\")\n",
        "sample_bigram(W2)\n",
        "print()\n",
        "print(\"Trigram samples:\")\n",
        "sample_trigram(W3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7k-xnwBwiAz"
      },
      "source": [
        "# Excercise 3\n",
        "\n",
        "* Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss.\n",
        "\n",
        "* What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve? **Answer:** not much. It's probably because I didn't try as many possibilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpeOVlQ21y1"
      },
      "source": [
        "## Tune regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G0Y0RPpgbiE",
        "outputId": "c0bdc03c-6c12-47bc-fa51-b486fab26c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model trigram\n",
            "new best_reg_rate=0.0010000000474974513, new best_loss=2.5970091819763184\n",
            "Training model trigram\n",
            "new best_reg_rate=0.5060403943061829, new best_loss=2.5839364528656006\n",
            "Training model trigram\n",
            "new best_reg_rate=1.0110808610916138, new best_loss=2.576690912246704\n",
            "Training model trigram\n",
            "new best_reg_rate=1.5161212682724, new best_loss=2.5721275806427\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Training model trigram\n",
            "Best reg_rate, loss is reg_rate=1.5161212682724, loss=2.5721275806427\n"
          ]
        }
      ],
      "source": [
        "reg_rates = torch.linspace(0.001, 50, steps=100)\n",
        "losses = []\n",
        "best_reg_rate = None\n",
        "best_loss = None\n",
        "\n",
        "for reg_rate in reg_rates:\n",
        "  W3 = torch.randn((trigram_input_size, trigram_output_size), generator=g, requires_grad=True)\n",
        "  train(split3['train']['xs'], split3['train']['ys'], W3, 50, 'trigram', reg_rate)\n",
        "  current_loss = loss(split3['dev']['xs'], split3['dev']['ys'], W3)\n",
        "  losses.append(current_loss)\n",
        "  if best_reg_rate is None or current_loss < best_loss:\n",
        "    best_reg_rate = reg_rate\n",
        "    best_loss = current_loss\n",
        "    print(f'new best_reg_rate={best_reg_rate}, new best_loss={best_loss}')\n",
        "\n",
        "print(f'Best reg_rate, loss is reg_rate={best_reg_rate}, loss={best_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSOWBt9P3CtX"
      },
      "source": [
        "## Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCQPaheb17PS",
        "outputId": "e1b37ce7-7fa3-4aff-cba2-240207337e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model trigram\n",
            "Test loss  2.6995866298675537\n"
          ]
        }
      ],
      "source": [
        "W3 = torch.randn((trigram_input_size, trigram_output_size), generator=g, requires_grad=True)\n",
        "train(split3['train']['xs'], split3['train']['ys'], W3, 50, 'trigram', best_reg_rate)\n",
        "print(\"Test loss \", loss(split3['test']['xs'], split3['test']['ys'], W3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-bXSIsYMWcp"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "* We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "EEbT5Kbn3RlZ"
      },
      "outputs": [],
      "source": [
        "# This will override _calculate_logits in the Core ML section\n",
        "\n",
        "# Explanations:\n",
        "# Let say W =\n",
        "#             [1, 2]\n",
        "#             [3, 4]\n",
        "#             [5, 6]\n",
        "#    and xs =\n",
        "#             [2, 1]\n",
        "#  => W[xs] =\n",
        "#             [5, 6]\n",
        "#             [3, 4]\n",
        "\n",
        "\n",
        "def _calculate_logits(xs, W):\n",
        "  return W[xs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXejrd1yMWrT"
      },
      "source": [
        "# Exercise 5\n",
        "\n",
        "* Look up and use F.cross_entropy instead. You should achieve the same result.\n",
        "* Can you think of why we'd prefer to use F.cross_entropy instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "VsPmZmZv3KY5"
      },
      "outputs": [],
      "source": [
        "# This will override _forward in the Core ML section\n",
        "\n",
        "def _forward(xs, ys, W, reg_matrix=None):\n",
        "  \"\"\"Neural net forward pass with F.cross_entropy\"\"\"\n",
        "\n",
        "  logits = _calculate_logits(xs, W)\n",
        "  loss = F.cross_entropy(logits, ys)\n",
        "  if reg_matrix is not None:\n",
        "    loss += reg_matrix\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0K5sYvdXWSJ"
      },
      "source": [
        "## Trigram Neural Net Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls9-l2UNXSyF",
        "outputId": "d0b845f8-f2e0-4d29-df8c-35d7076fb329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model trigram\n",
            "Neural network solution loss\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.1552374362945557"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs3, ys3 = get_trigram_dataset(words)\n",
        "split3 = split_dataset(xs3, ys3, 1, 0, 0)\n",
        "W3 = torch.randn((trigram_input_size, trigram_output_size), generator=g, requires_grad=True)\n",
        "train(split3['train']['xs'], split3['train']['ys'], W3, 500, 'trigram')\n",
        "print(\"Neural network solution loss\")\n",
        "loss(xs3, ys3, W3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
